{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi v3 tutorial \n",
    "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "**env.reset**: Resets the environment and returns a random initial state.\n",
    "\n",
    "**env.step(action)**: Step the environment by one timestep. Returns<br/>\n",
    "- observation: Observations of the environment\n",
    "- reward: If your action was beneficial or not\n",
    "- done: Indicates if we have successfully picked up and dropped off a passenger, also called one episode<br/>\n",
    "- info: Additional info such as performance and latency for debugging purposes<br/>\n",
    "    \n",
    "**env.render**: Renders one frame of the environment (helpful in visualizing the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "action space Discrete(6)\n",
      "state space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# reset enviroment to a new, random state\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"action space {}\".format(env.action_space))\n",
    "print(\"state space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **filled square represents** the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The **pipe (\"|\")** represents a wall which the taxi cannot cross.\n",
    "- **R, G, Y, B** are the possible pickup and destination locations. The **blue letter** represents the current passenger pick-up location, and the **purple letter** is the current destination.\n",
    "\n",
    "6 action space and state space of size 500.\n",
    "- identify a state uniquely by assigning a unique number to every possible state\n",
    "- RL learns to choose an action number from 0-5(0=south, 1=north, 2=east, 3=west, 4=pickup, 5=dropoff)\n",
    "- 500 states = encoding of the **taxi's location, the passenger's location, and the destination location**\n",
    "- The optimal action for each state is the action that has the highest cumulative long-term reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3,1,2,0) #taxi at row 3 column1, passenger is at location2, destination location is 0\n",
    "print (\"state: \",state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate a number corressponding to a state 0 - 499 > **328**\n",
    "- sen enviroment's state manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  248\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(2,2,2,0) #taxi at row 2 column2, passenger is at location3, destination location is 0\n",
    "print (\"state: \",state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate a number corressponding to a state 0 - 499 > **248**\n",
    "- sen enviroment's state manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward Table\n",
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure `{action: [(probability, nextstate, reward, done)]}`\n",
    "- 0-5 taxi can perfrom\n",
    "- `probability` is always 1.0\n",
    "- `nextstate` state if a agent take an action\n",
    " - **-1** all movement, **-10** \n",
    " - **20** passenger and right destination at the dropoff action\n",
    "- `done` dropped off a passenger in the right direction/ the end of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the environment without Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 2324\n",
      "Penalties incurred: 778\n"
     ]
    }
   ],
   "source": [
    "env.s=328\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0,0 \n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == -10:\n",
    "        penalties +=1\n",
    "        \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    epochs +=1\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p` reward table \n",
    "`env.action_space.sample()` automatically selects one random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Timestep: 2324\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        # print(frame['frame'].getvalue())\n",
    "        env.s = frame['state']\n",
    "        env.render()\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes thousands of timesteps and makes lots of wrong drop offs for one passenger to the right destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Solving the environment  with Reinforcement Learning</span>\n",
    "\n",
    "`p` reward table\n",
    "**Q-values** `(state, action)` updating reward in the Q-table to rember if an action is beneficial\n",
    "   - quality of an action taken from that state. \n",
    "   - **Better Q-values** imply better chances of getting greater rewards.\n",
    "   - initialize to an arbitary value\n",
    "\n",
    "> Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n",
    "- α (alpha) is the learning rate( 0 < α ≤ 1), Qvalues that being updated every iteration\n",
    "- γ (gamma) is the discount factor (0 ≤ γ ≤ 1), how much importance we want to gitve to future rewards. \n",
    "- 1 logn-term effecitve 0 - immediate reward \n",
    "\n",
    "#### Q-table ###\n",
    "- Matrix has a row for every state(500) and column for every action(6)\n",
    "- Initinalize to 0, and values are updated after training\n",
    "- same dimensions as the reward table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
