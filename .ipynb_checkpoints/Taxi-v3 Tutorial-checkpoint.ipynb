{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi v3 tutorial \n",
    "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "**env.reset**: Resets the environment and returns a random initial state.\n",
    "\n",
    "**env.step(action)**: Step the environment by one timestep. Returns<br/>\n",
    "- observation: Observations of the environment\n",
    "- reward: If your action was beneficial or not\n",
    "- done: Indicates if we have successfully picked up and dropped off a passenger, also called one episode<br/>\n",
    "- info: Additional info such as performance and latency for debugging purposes<br/>\n",
    "    \n",
    "**env.render**: Renders one frame of the environment (helpful in visualizing the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "action space Discrete(6)\n",
      "state space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# reset enviroment to a new, random state\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"action space {}\".format(env.action_space))\n",
    "print(\"state space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **filled square represents** the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The **pipe (\"|\")** represents a wall which the taxi cannot cross.\n",
    "- **R, G, Y, B** are the possible pickup and destination locations. The **blue letter** represents the current passenger pick-up location, and the **purple letter** is the current destination.\n",
    "\n",
    "6 action space and state space of size 500.\n",
    "- identify a state uniquely by assigning a unique number to every possible state\n",
    "- RL learns to choose an action number from 0-5(0=south, 1=north, 2=east, 3=west, 4=pickup, 5=dropoff)\n",
    "- 500 states = encoding of the **taxi's location, the passenger's location, and the destination location**\n",
    "- The optimal action for each state is the action that has the highest cumulative long-term reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3,1,2,0) #taxi at row 3 column1, passenger is at location2, destination location is 0\n",
    "print (\"state: \",state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate a number corressponding to a state 0 - 499 > **328**\n",
    "- sen enviroment's state manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  248\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(2,2,2,0) #taxi at row 2 column2, passenger is at location3, destination location is 0\n",
    "print (\"state: \",state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate a number corressponding to a state 0 - 499 > **248**\n",
    "- sen enviroment's state manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward Table\n",
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure `{action: [(probability, nextstate, reward, done)]}`\n",
    "- 0-5 taxi can perfrom\n",
    "- `probability` is always 1.0\n",
    "- `nextstate` state if a agent take an action\n",
    " - **-1** all movement, **-10** \n",
    " - **20** passenger and right destination at the dropoff action\n",
    "- `done` dropped off a passenger in the right direction/ the end of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the environment without Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 121\n",
      "Penalties incurred: 39\n"
     ]
    }
   ],
   "source": [
    "env.s=328\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0,0 \n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == -10:\n",
    "        penalties +=1\n",
    "        \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    epochs +=1\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p` reward table \n",
    "`env.action_space.sample()` automatically selects one random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Timestep: 121\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        # print(frame['frame'].getvalue())\n",
    "        env.s = frame['state']\n",
    "        env.render()\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes thousands of timesteps and makes lots of wrong drop offs for one passenger to the right destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Solving the environment  with Reinforcement Learning</span>\n",
    "\n",
    "`p` reward table\n",
    "**Q-values** `(state, action)` updating reward in the Q-table to rember if an action is beneficial\n",
    "   - quality of an action taken from that state. \n",
    "   - **Better Q-values** imply better chances of getting greater rewards.\n",
    "   - initialize to an arbitary value\n",
    "\n",
    "> Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n",
    "- α (alpha) is the learning rate( 0 < α ≤ 1), Qvalues that being updated every iteration\n",
    "- γ (gamma) is the discount factor (0 ≤ γ ≤ 1), how much importance we want to gitve to future rewards. \n",
    "- 1 logn-term effecitve 0 - immediate reward \n",
    "\n",
    "#### Q-table ###\n",
    "- Matrix has a row for every state(500) and column for every action(6)\n",
    "- Initinalize to 0, and values are updated after training\n",
    "- same dimensions as the reward table\n",
    "\n",
    "### Random & Optimal ###\n",
    "- Q-values tend to converge serving the most optimal action (exploitation) always taking the same route, possibly overfitting\n",
    "- ϵ  \"epsilon\" lower epsilon - more penalties (Random)\n",
    "\n",
    "### training algorithm ###\n",
    "- agent explores the env over 1000 episodes\n",
    "- `while not done` pick a random action or exploit Qvalue\n",
    " - by `epsilon` value comparing to the `random.uniform(0,1)` **returns an arbitrary number between 0 and 1**\n",
    "- execute the chose action to obtain `next_state` and `reward`\n",
    "- calculate the maximum Q-value for the actions `next_state`\n",
    "- update Q-value `next_q_value`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning finished. \n",
      "\n",
      "Wall time: 39.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" Training the agent \"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "#For plotting metrics\n",
    "all_epocs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range (1, 100001):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0,0,0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() #Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) #Exploit learned values\n",
    "        \n",
    "        next_state, reward, done, infor = env.step(action)\n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1-alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties +=1\n",
    "        \n",
    "        state = next_state\n",
    "        epochs +=1\n",
    "        \n",
    "    if 1 % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        \n",
    "print(\"Traning finished. \\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Q-Table established over 100,000 episodes.\n",
    "\n",
    "The max Q-Value is \"north\" (-2.2) > Q-learning has effectively learned the best action to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.41043841,  -2.27325184,  -2.38028529,  -2.35555588,\n",
       "       -10.60012791, -10.41699021])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the agent performance\n",
    "next action is always selected using the best Q-Value\n",
    "**Performance improved significantly and it incurred no penalties**\n",
    "correct pickup/dropoff actions with 100 different passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.44\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Q-learning agent vs no Reinforcement Learning\n",
    "\n",
    "Initially the agent will make errors but, once it made a Q-table, it can act wisely maximizing the rewards\n",
    "\n",
    "1. Average number of penalites per episode:\n",
    "The smaller, the better the performance\n",
    "\n",
    "2. Average number of timesteps per trip:\n",
    "The smaller number = minimum steps to reach the destination\n",
    "\n",
    "3. Average rewards per move\n",
    "The larger the rwards = right ting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and optimizations\n",
    "\n",
    "alpha, gamma, epsilon : intuition, hit and trial >> better ways to come up with good values.\n",
    "\n",
    "1. **α(the learning rate)** should **decrease** as you continue to gain a larger and larger knowledge base.\n",
    "2. **γ(discount factor)**: as you get closer and closer to the deadline, your preference for near-term reward should **increase**, as you won't be around long enough to get the long-term reward, which means your gamma should decrease.\n",
    "3. **ϵ(add random)**: as we develop our strategy, we have less need of exploration and more exploitation to get more utility from our policy, so as **trials increase, epsilon should decrease.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
